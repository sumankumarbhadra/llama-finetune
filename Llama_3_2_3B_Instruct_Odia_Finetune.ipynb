{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjNq9TjBg__N"
      },
      "source": [
        "# Llama-3.2-3B-Instruct Fine-tuning for Odia Language\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhpAskYghLqt"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPCA1q7YgSWN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWMrXrwShrIr"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQSg5UHbtsyi"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtnR6As9t3av"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_jYzTttQ_k7"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import random\n",
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20qLpDHKt5Vi"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"sumankumarbhadra/alpaca-odia\", split=\"train\")\n",
        "dataset = dataset.to_pandas()\n",
        "\n",
        "print(f\"Original dataset size: {len(dataset)}\")\n",
        "print(\"Sample data:\")\n",
        "print(dataset.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOQ4lxI0ufNJ"
      },
      "outputs": [],
      "source": [
        "# Data augmentation function\n",
        "def augment_dataset(df):\n",
        "    augmented_data = []\n",
        "\n",
        "    # Prompt templates for English instructions\n",
        "    odia_prompt_templates = [\n",
        "        \"Answer in Odia: {instruction}\",\n",
        "        \"Provide your response in Odia: {instruction}\",\n",
        "        \"Reply in Odia language: {instruction}\",\n",
        "        \"Give an Odia response to this question: {instruction}\",\n",
        "        \"Write your answer in Odia: {instruction}\",\n",
        "        \"Respond using Odia language: {instruction}\",\n",
        "        \"{instruction} Answer in Odia.\",\n",
        "        \"{instruction} Respond in Odia only.\",\n",
        "        \"Express your answer in Odia: {instruction}\",\n",
        "        \"Present the information in Odia: {instruction}\",\n",
        "        \"Use Odia language for this response: {instruction}\",\n",
        "        \"Output in Odia: {instruction}\",\n",
        "        \"{instruction} (Respond in Odia)\",\n",
        "        \"{instruction} Please answer using Odia.\",\n",
        "        \"Your task: {instruction} Format: Odia language\",\n",
        "        \"Communicate this in Odia: {instruction}\",\n",
        "        \"Craft an Odia response to: {instruction}\",\n",
        "        \"{instruction} → Odia response required\",\n",
        "        \"Using Odia script, answer: {instruction}\",\n",
        "        \"In ଓଡ଼ିଆ only: {instruction}\",\n",
        "        \"{instruction} Return information in Odia.\",\n",
        "        \"Answer this question with Odia text: {instruction}\",\n",
        "        \"Formulate your response in ଓଡ଼ିଆ: {instruction}\",\n",
        "        \"Odia output requested: {instruction}\",\n",
        "        \"{instruction} Your response should be in the Odia language.\",\n",
        "        \"Generate Odia text for: {instruction}\",\n",
        "        \"{instruction} Convey this information in Odia.\"\n",
        "    ]\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        # Case 1: English instruction + English input -> Odia output\n",
        "        english_inst = row['original_instruction']\n",
        "        english_inp = row['original_input'] if pd.notna(row['original_input']) else \"\"\n",
        "        odia_out = row['output']\n",
        "\n",
        "        # Format the instruction with a random template\n",
        "        formatted_inst = random.choice(odia_prompt_templates).format(instruction=english_inst)\n",
        "\n",
        "        augmented_data.append({\n",
        "            \"instruction\": formatted_inst,\n",
        "            \"input\": english_inp,\n",
        "            \"output\": odia_out\n",
        "        })\n",
        "\n",
        "        # Case 2: English instruction + Odia input -> Odia output\n",
        "        if pd.notna(row['input']):  # If Odia input exists\n",
        "            augmented_data.append({\n",
        "                \"instruction\": english_inst,\n",
        "                \"input\": row['input'],\n",
        "                \"output\": odia_out\n",
        "            })\n",
        "\n",
        "        # Case 3: Odia instruction + Odia input -> Odia output\n",
        "        if pd.notna(row['instruction']) and pd.notna(row['input']):\n",
        "            augmented_data.append({\n",
        "                \"instruction\": row['instruction'],\n",
        "                \"input\": row['input'],\n",
        "                \"output\": odia_out\n",
        "            })\n",
        "        elif pd.notna(row['instruction']):\n",
        "            augmented_data.append({\n",
        "                \"instruction\": row['instruction'],\n",
        "                \"input\": \"\",\n",
        "                \"output\": odia_out\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(augmented_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S515BV6ZBxRh"
      },
      "outputs": [],
      "source": [
        "# Augment the dataset\n",
        "augmented_df = augment_dataset(dataset)\n",
        "print(f\"Augmented dataset size: {len(augmented_df)}\")\n",
        "\n",
        "# Convert to HF dataset\n",
        "augmented_dataset = Dataset.from_pandas(augmented_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_cwTZstDK8p"
      },
      "outputs": [],
      "source": [
        "# Convert to conversation format\n",
        "def convert_to_conversations(examples):\n",
        "    conversations = []\n",
        "\n",
        "    for i in range(len(examples[\"instruction\"])):\n",
        "        messages = []\n",
        "\n",
        "        # Add user message (instruction + input)\n",
        "        user_msg = examples[\"instruction\"][i]\n",
        "        if examples[\"input\"][i] and examples[\"input\"][i].strip():\n",
        "            user_msg += \"\\n\\n\" + examples[\"input\"][i]\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "\n",
        "        # Add assistant message (output)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": examples[\"output\"][i]})\n",
        "\n",
        "        conversations.append(messages)\n",
        "\n",
        "    return {\"conversations\": conversations}\n",
        "\n",
        "# Convert dataset to conversation format\n",
        "conversation_dataset = augmented_dataset.map(convert_to_conversations, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzmaLqUdDSMV"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "# Set up tokenizer with chat template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",  # Use llama-3.1 template which works for 3.2 as well\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Format the prompts\n",
        "formatted_dataset = conversation_dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Show sample formatted text\n",
        "print(\"\\nSample formatted text:\")\n",
        "print(formatted_dataset[0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYzS8stbDaCR"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = formatted_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 32,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        # max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XWi9j5dDi_z"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBtZNzenEHWz"
      },
      "source": [
        "Verify masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk1li_SSD5Se"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruUMdo3qD_tK"
      },
      "outputs": [],
      "source": [
        "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
        "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-CfGk5VJEEkh"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjVGG23qO5rd"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAxCsCLySr-8"
      },
      "outputs": [],
      "source": [
        "# Test inference\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Set tokenizer for generation\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.1\",\n",
        ")\n",
        "\n",
        "# Test with a new example\n",
        "test_examples = [\n",
        "    \"Translate the following to Odia: Hello, how are you today?\",\n",
        "    \"ଆପଣଙ୍କ ନାମ କଣ?\",\n",
        "    \"Tell me about Odisha in Odia language.\"\n",
        "]\n",
        "\n",
        "for test_example in test_examples:\n",
        "    messages = [{\"role\": \"user\", \"content\": test_example}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    from transformers import TextStreamer\n",
        "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "    print(f\"\\nUser: {test_example}\")\n",
        "    print(\"Assistant: \", end=\"\")\n",
        "\n",
        "    _ = model.generate(\n",
        "        input_ids=inputs,\n",
        "        streamer=text_streamer,\n",
        "        max_new_tokens=128,\n",
        "        use_cache=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DoVPRHzS60z"
      },
      "source": [
        "## Save The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3IWQqzBTEdz"
      },
      "source": [
        "## Push to Hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F2cKU0aTAhR"
      },
      "outputs": [],
      "source": [
        "model_name = \"Llama-3.2-3B-Instruct-Odia\"\n",
        "HF_USERNAME = \"sumankumarbhadra\"\n",
        "HF_MODEL_NAME = f\"{HF_USERNAME}/{model_name}\"\n",
        "\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "\n",
        "# Merge to 16bit\n",
        "model.save_pretrained_merged(model_name, tokenizer, save_method = \"merged_16bit\",)\n",
        "model.push_to_hub_merged(HF_MODEL_NAME, tokenizer, save_method = \"merged_16bit\", token = hf_token)\n",
        "\n",
        "# Just LoRA adapters\n",
        "model.save_pretrained_merged(model_name, tokenizer, save_method = \"lora\",)\n",
        "model.push_to_hub_merged(HF_MODEL_NAME, tokenizer, save_method = \"lora\", token = hf_token)\n",
        "\n",
        "model.push_to_hub_gguf(\n",
        "        HF_MODEL_NAME,\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q5_k_m\", \"q8_0\"],\n",
        "        token = hf_toke\n",
        "    )\n",
        "\n",
        "print(f\"Model pushed to Hugging Face Hub as {HF_MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCEYBhspR_AT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}